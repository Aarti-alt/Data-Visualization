{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_text_Visualization_session2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r81Is343Benc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "78fa9a2f-2908-4adb-9d09-1af5d67a7a14"
      },
      "source": [
        "#REGULAR EXPRESSIONS\n",
        "#. - match any character except newline\n",
        "#\\w - match word\n",
        "#\\d - match digit\n",
        "#\\s - match whitespace\n",
        "#\\W - match not word\n",
        "#\\D - match not digit\n",
        "#\\S - match not whitespace\n",
        "#[abc] - match any of a, b, or c\n",
        "#[^abc] - not match a, b, or c\n",
        "#[a-g] - match a character between a & g\n",
        "import re\n",
        "sentence = \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having  nothing to do: once or twice she had peeped into the book her sister was reading, but it  had no pictures or conversations in it, 'and what is the use of a book,' thought Alice  'without pictures or conversations?'.\"\n",
        "pattern = r\"[^\\w]\"\n",
        "print(re.sub(pattern, \" \", sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice was beginning to get very tired of sitting by her sister on the bank  and of having  nothing to do  once or twice she had peeped into the book her sister was reading  but it  had no pictures or conversations in it   and what is the use of a book   thought Alice   without pictures or conversations   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoJ6WE_iCDhg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a8dbe307-edee-4fc7-96f3-44300b17174f"
      },
      "source": [
        "#import the library for natural language processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBpjzk7-DdcB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "4d0951d9-6765-4912-e16e-198ecb81f0ab"
      },
      "source": [
        "#BAG OF WORDS\n",
        "#Machine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.\n",
        "#The bag-of-words model is a popular and simple feature extraction technique used when we work with text. \n",
        "#It describes the occurrence of each word within a document.\n",
        "#To use this model, we need to:\n",
        "#Design a vocabulary of known words (also called tokens)\n",
        "#Choose a measure of the presence of known words\n",
        "#Any information about the order or structure of words is discarded. That’s why it’s called a bag of words.\n",
        "# Import the libraries we need\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having  nothing to do: once or twice she had peeped into the book her sister was reading but it  had no pictures or conversations in it. and what is the use of a book, thought Alice without pictures or conversations?.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "    print(sentence);\n",
        "    print()\n",
        "\n",
        "\n",
        "# Step 2. Design the Vocabulary\n",
        "# The default token pattern removes tokens of a single character. That's why we don't have the \"I\" and \"s\" tokens in the output\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Step 3. Create the Bag-of-Words Model\n",
        "bag_of_words = count_vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Show the Bag-of-Words Model as a pandas DataFrame\n",
        "feature_names = count_vectorizer.get_feature_names()\n",
        "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice was beginning to get very tired of sitting by her sister on the bank, and of having  nothing to do: once or twice she had peeped into the book her sister was reading but it  had no pictures or conversations in it.\n",
            "\n",
            "and what is the use of a book, thought Alice without pictures or conversations?.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alice</th>\n",
              "      <th>and</th>\n",
              "      <th>bank</th>\n",
              "      <th>beginning</th>\n",
              "      <th>book</th>\n",
              "      <th>but</th>\n",
              "      <th>by</th>\n",
              "      <th>conversations</th>\n",
              "      <th>do</th>\n",
              "      <th>get</th>\n",
              "      <th>had</th>\n",
              "      <th>having</th>\n",
              "      <th>her</th>\n",
              "      <th>in</th>\n",
              "      <th>into</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>no</th>\n",
              "      <th>nothing</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>once</th>\n",
              "      <th>or</th>\n",
              "      <th>peeped</th>\n",
              "      <th>pictures</th>\n",
              "      <th>reading</th>\n",
              "      <th>she</th>\n",
              "      <th>sister</th>\n",
              "      <th>sitting</th>\n",
              "      <th>the</th>\n",
              "      <th>thought</th>\n",
              "      <th>tired</th>\n",
              "      <th>to</th>\n",
              "      <th>twice</th>\n",
              "      <th>use</th>\n",
              "      <th>very</th>\n",
              "      <th>was</th>\n",
              "      <th>what</th>\n",
              "      <th>without</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   alice  and  bank  beginning  book  but  ...  twice  use  very  was  what  without\n",
              "0      1    1     1          1     1    1  ...      1    0     1    2     0        0\n",
              "1      1    1     0          0     1    0  ...      0    1     0    0     1        1\n",
              "\n",
              "[2 rows x 39 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jZ4QUnD2FN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "b5907d98-a2fa-4ebe-bd2f-533cf3f414d9"
      },
      "source": [
        "#Term Frequency (TF): a scoring of the frequency of the word in the current document.\n",
        "#Inverse Term Frequency (ITF): a scoring of how rare the word is across documents.\n",
        "# TFIDF = TF*IDF\n",
        "#TFIDF code \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "values = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Show the Model as a pandas DataFrame\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "pd.DataFrame(values.toarray(), columns = feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alice</th>\n",
              "      <th>and</th>\n",
              "      <th>bank</th>\n",
              "      <th>beginning</th>\n",
              "      <th>book</th>\n",
              "      <th>but</th>\n",
              "      <th>by</th>\n",
              "      <th>conversations</th>\n",
              "      <th>do</th>\n",
              "      <th>get</th>\n",
              "      <th>had</th>\n",
              "      <th>having</th>\n",
              "      <th>her</th>\n",
              "      <th>in</th>\n",
              "      <th>into</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>no</th>\n",
              "      <th>nothing</th>\n",
              "      <th>of</th>\n",
              "      <th>on</th>\n",
              "      <th>once</th>\n",
              "      <th>or</th>\n",
              "      <th>peeped</th>\n",
              "      <th>pictures</th>\n",
              "      <th>reading</th>\n",
              "      <th>she</th>\n",
              "      <th>sister</th>\n",
              "      <th>sitting</th>\n",
              "      <th>the</th>\n",
              "      <th>thought</th>\n",
              "      <th>tired</th>\n",
              "      <th>to</th>\n",
              "      <th>twice</th>\n",
              "      <th>use</th>\n",
              "      <th>very</th>\n",
              "      <th>was</th>\n",
              "      <th>what</th>\n",
              "      <th>without</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.306186</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.306186</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.306186</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>0.102062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      alice       and      bank  ...       was      what   without\n",
              "0  0.204124  0.204124  0.102062  ...  0.204124  0.102062  0.102062\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[2 rows x 39 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-GCuVE_D-aJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "fc82d65c-1072-4c24-894b-4d2de8b5a607"
      },
      "source": [
        "# N GRAM\n",
        "# An n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). \n",
        "#In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, \n",
        "#a trigram is a sequence of three words etc. The “n” in the “n-gram” refers to the number of the grouped words.\n",
        "#The office building is open today\n",
        "#All the bigrams are:\n",
        "#the office\n",
        "#office building\n",
        "#building is\n",
        "#is open\n",
        "#open today\n",
        "#The bag-of-bigrams is more powerful than the bag-of-words approach.\n",
        "# Importing libraries \n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
        "import pandas as pd\n",
        "text = \"So she was considering in her own mind (as well as she could, for the hot day made  her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would  be worth the trouble of getting up and picking the daisies, when suddenly a White  Rabbit with pink eyes ran close by her.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for sentence in sentences:\n",
        "    print(sentence);\n",
        "    print()\n",
        "#Bigram generator\n",
        "vectorizer = CountVectorizer(ngram_range =(2, 2)) \n",
        "X1 = vectorizer.fit_transform(sentences)  \n",
        "features = (vectorizer.get_feature_names())\n",
        " \n",
        "print(\"\\n\\nX1 : \\n\", features)\n",
        "print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
        "\n",
        "#Getting TF-IDF Score here\n",
        "vectorizer = TfidfVectorizer(ngram_range = (2, 2)) \n",
        "X2 = vectorizer.fit_transform(sentences) \n",
        "scores = (X2.toarray()) \n",
        "print(\"\\n\\nScores : \\n\", scores) \n",
        "\n",
        "#For getting top ranking values\n",
        "sums = X2.sum(axis = 0) \n",
        "data1 = [] \n",
        "for col, term in enumerate(features): \n",
        "    data1.append( (term, sums[0, col] )) \n",
        "ranking = pd.DataFrame(data1, columns = ['term', 'rank']) \n",
        "words = (ranking.sort_values('rank', ascending = False)) \n",
        "print (\"\\n\\nWords : \\n\", words.head(7)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So she was considering in her own mind (as well as she could, for the hot day made  her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would  be worth the trouble of getting up and picking the daisies, when suddenly a White  Rabbit with pink eyes ran close by her.\n",
            "\n",
            "\n",
            "\n",
            "X1 : \n",
            " ['and picking', 'and stupid', 'as she', 'as well', 'be worth', 'by her', 'chain would', 'close by', 'considering in', 'could for', 'daisies when', 'daisy chain', 'day made', 'eyes ran', 'feel very', 'for the', 'getting up', 'her feel', 'her own', 'hot day', 'in her', 'made her', 'making daisy', 'mind as', 'of getting', 'of making', 'own mind', 'picking the', 'pink eyes', 'pleasure of', 'rabbit with', 'ran close', 'she could', 'she was', 'sleepy and', 'so she', 'stupid whether', 'suddenly white', 'the daisies', 'the hot', 'the pleasure', 'the trouble', 'trouble of', 'up and', 'very sleepy', 'was considering', 'well as', 'when suddenly', 'whether the', 'white rabbit', 'with pink', 'worth the', 'would be']\n",
            "\n",
            "\n",
            "X1 : \n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "\n",
            "\n",
            "Scores : \n",
            " [[0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056 0.13736056\n",
            "  0.13736056 0.13736056 0.13736056 0.13736056 0.13736056]]\n",
            "\n",
            "\n",
            "Words : \n",
            "            term      rank\n",
            "0   and picking  0.137361\n",
            "27  picking the  0.137361\n",
            "29  pleasure of  0.137361\n",
            "30  rabbit with  0.137361\n",
            "31    ran close  0.137361\n",
            "32    she could  0.137361\n",
            "33      she was  0.137361\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}